---
title: "Multiobjective Evolutionary Algorithms"
author: "Thorben Hellweg"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Multiobjective Evolutionary Algorithms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Multiobjective Optimization Problems

With **ecr**, both single- and multiobjective optimization problems can be addressed. In the former, an attempt is made to find to find a single solution that maximises a fitness value that corresponds directly to a single underlying measure of quality. Often, however, we look at problems in which we try to optimize different (conflicting) goals at the same time. Examples of these so-called multiobjective problems (MOPs) include:

* Hotel search: Close to the beach and at the same time reasonably priced
* Feature Selection in Machine Learning: Few Features with High Accurancy
* Real estate purchase: location, price, number of rooms, square meter, ...

Different objectives can also mean different optimization goals: When buying a new apartment, one hopes for a low price (minimization problem) and a large living space (maximization problem).

In the following, we will look at the example of Feature Selection to see how multiobjective problems can be solved with **ecr**.

For this purpose, we will iteratively train a RandomForest-Classifier on the Wisconsin Breast Cancer data set and analyze the effect on the accuracy of prediction by including or omitting features of the data set. The goal is to achieve maximum accuracy with a minimum number of features. To train the RandomForest-Classifier we use the R-Package mlr, the BreastCancer data set is taken from the mlbench-Package.

```{r breastCancer, message=FALSE}
library(ecr)
library(mlr)
library(mlbench)
library(randomForest)
data("BreastCancer")
summary(BreastCancer)
```

First we remove the observations with missing data (see Bare.nuclei's 16 NA entries) and the Id column which is irrelevant for the training of the model. The data set is then divided into a feature data set and a target data set. The prediction target is the column "Class".

```{r}
cancer = BreastCancer[,2:11]
cancer <- cancer[!(rowSums(is.na(cancer)) > 0),]
cancerFeatures = cancer[,1:9]
cancerTarget = cancer[,10]
```

Next, we define the fitness function. First a few conceptual considerations: The fitness of an individual results from the number of features used for prediction and the accuracy of the prediction. In order to determine the accuracy, the model must be trained on the features encoded in the individual. Consequently, each time the fitness function is called, the model is first trained and then the fitness is determined. We use a resampling strategy to determine the performance of the learning algorithm on the selected features. The fitness corresponds to the average accuracy of the individual samples.

```{r}
  fitness.fun = function(ind) {
    ind = as.logical(ind)
    # all features deselected is not a supported solution.
    # Thus, we set the accurancy to 0 and number of features to its maximum.
    if (!any(ind))
      return(c(0, length(ind)))
    # add target column to individuum
    task = makeClassifTask(data = cancer[, c(ind,TRUE)],
                           target = "Class",
                           id = "Cancer")
    # Subsampling with 5 iterations and default split ratio 2/3
    rdesc = makeResampleDesc("Subsample", iters = 5)
    # Classification tree
    lrn = makeLearner("classif.randomForest")
    r = do.call(resample, list(lrn, task, rdesc, list(acc), show.info = FALSE))
    measure = r$aggr[[1]]
    nFeatures = sum(ind[1:length(ind)] == 1)
    return(c(measure, nFeatures))
  }
```

## Black-box approach 
Since ecr supports multiobjective optimization as a standard task, we can use the black-box function ecr() to execute the feature selection. We decide to use an evolutionary $(5 + 10)$-strategy, i.e., an algorithm that keeps a population of size mu = 5, in each generation creates lambda = 10 offspring by variation and selects the best mu out of mu + lambda individuals to survive. In the context of our multiobjective optimization problem, two important arguments should be noticed. First, *n.objectives* has to reflect the number of objectives and *minimize* has to be a vector of length = n.objectives, indicating for each objective as to whether it should be minimized or maximised. For the problem at hand, our two objectives (Accurancy, Number of Features) are to be maximised and minimized respectively. 
```{r}
MU = 5; LAMBDA = 8; MAX.ITER = 2; N.BITS = ncol(cancerFeatures);
res = ecr(fitness.fun = fitness.fun, 
            n.objectives = 2L, 
            minimize = c(FALSE, TRUE), 
            representation = "binary",
            n.bits = N.BITS, 
            mu = MU, 
            lambda = LAMBDA, 
            survival.strategy = "plus",
            mutator = setup(mutBitflip, p = 1 / N.BITS),
            p.mut = 0.3, 
            p.recomb = 0.7, 
            terminators = list(stopOnIters(MAX.ITER)),
            log.pop = TRUE,
            initial.solutions = list(rep(1,N.BITS)))
```

The resulting pareto set consits of all nondominated solutions and can be plotted by using *plotFront*.

```{r, message = FALSE}
res$last.population
res
plotFront(res$pareto.front)
```
